---
title: "Take home exercise 2"
---

---
title: "Take home Exercise 2"
author: "Mah Lian Khye"
date: "14 December 2023"
date-modified: "last-modified"

format: 
  html:
    self_contained: false
    code-fold: true
    code-summary: "code chunk"
execute:
  echo: true # all code chunks will appear
  eval: true # all code chunks will run live (be evaluated)
---

# Overview

Organising bodies will have to consider passenger usage to and from locations when adding, changing or removing current transportation routes. Any alterations to present routes can lead to passengers having to seek alternate travelling options that can impact travel time and distance. To minimise the impact, the travelling routes taken will have to be surveyed thorugh commuters survey. However this form of sampling can be inefficient and will easily be out of date. The use of passenger tracking methods such as GPS or smart card boarding and alighting locations may be used as a substitute that can generate data more readily.

# Objective

The main objectives of this exercise is to analyse the boarding and alighting locations of passengers of a time interval. This analysis will allow us to track the factors that affect urban mobility so that route usage may be analyse for future route planning.

# Tasks

The tasks of this exercise are as follow:

-   Derive an analytical hexagon map to be used as a traffic analysis zone
-   Construct an O-D matrix of commuter flow for weekday evenings from 5pm to 8pm
-   Display the O-D flows
-   Assemble at least 3 propulsive and 3 attractiveness variables
-   Compute a distance matrix
-   Calibrate spatial interactive models and present the modelling results using geovisualisation and graphical visualisation methods

# Importing packages and data

::: panel-tabset
## Import packages

Firstly, we will import the various packages that we will need for this exercise.

```{r}
#| code-fold: false
pacman::p_load(tmap, sf, sp, DT, 
               performance, reshape2,
               tidyverse, dplyr,  stplanr, ggpubr)
```

::: callout-note
### Package functions

-   **tmap**: choropleth map visualisation
-   **sf**: package containing functions to support simple features and standardised a way to encode spatial vector data
-   **DT**: for creating DataTables
-   **performance**: for assessing model quality
-   **reshape2**: for reshaping data
-   **tidyverse**: for manipulating and data presentations
-   **dplyr**: for working with dataframes
-   **stplanr**: for creating mobility lines
-   **ggpubr**: for manipulating plots
:::

## Import data

Next we will import the data to be used for analysis.

::: panel-tabset
### **Loading aspatial table**

Here we will import the ridership of the different bus stops in Oct 2023.

```{r}
#| code-fold: false
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```

Next we will extract the ridership for weekdays from 5pm to 8pm only.

```{r}
#| code-fold: false
weekdayPM <- odbus %>% 
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 & TIME_PER_HOUR <= 20) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

```

### Loading geospatial data

Next we will import all of the bus stops and their coordinates and attached it to the *busstop* variable.

```{r}
#| code-fold: false
busstop <- st_read(dsn = "data/geospatial", layer = "BusStop") %>%
  st_transform(crs = 3414)
```

We will also import all of the attractive and propulsive factors that we will use for subsequent modelling analysis.

::: panel-tabset
#### Attractive factors

For attractive factors we will be using HDBs, retail outlets and train station exits as people either take buses to return home directly or to the train stations for their subsequent journey after work. There might be people who choose to do shopping after work or dinner, therefore retail stores have been chosen as a attractive factor. Here we will import the aspatial data for them.

For HDB data we will have to process them from latitude and longitude points to projected coordinate system.

```{r}
#| code-fold: false
hdb <- read_csv("data/aspatial/hdb.csv")
hdb_sf <- st_as_sf(hdb, coords = c("lng", "lat"), crs = st_crs(4326)) %>%
  st_transform(crs = 3414)

```

We can import the retail stores and train station exits data directly as they are already in the correct coordinate system that we will be using in this exercise.

```{r}
#| code-fold: false

retail_sf <- st_read(dsn = "data/geospatial", 
                layer = "Retails") %>%
  st_transform(crs=3414)


stn_exit_sf <- st_read(dsn = "data/geospatial", 
                layer = "Train_Station_Exit_Layer") %>%
  st_transform(crs=3414)
```

::: callout-note
### Coordinate systems

Remember to use the correct coordinate system so that data can be matched correctly! More information for the system used can be found [here](https://epsg.io/3414).
:::

#### Propulsive factors

The propulsive factors chosen are schools, businesses and financial services. They are identified as so as people leave work and school in the evening.

For school data we will have to process them from latitude and longitude points to projected coordinate system.

```{R}
schools <- read_csv("data/aspatial/schools.csv")
schools <- schools %>%
  rename("latitude" = "results.LATITUDE",
         "longitude" = "results.LONGITUDE") %>%
  select(postal_code, school_name, latitude, longitude)
schools_sf <- st_as_sf(schools, coords = c("longitude", "latitude"), crs = st_crs(4326)) %>%
  st_transform(crs = 3414)
```

For financial services and businesses, we can import the geospatial data directly.

```{r}

fin_sf <- st_read(dsn = "data/geospatial", 
                layer = "FinServ") %>%
  st_transform(crs=3414)
biz_sf <- st_read(dsn = "data/geospatial", 
                layer = "Business") %>%
  st_transform(crs=3414)

```
:::
:::
:::

We will first rename the bus stop column title for easier data joining.

```{r}
colnames(busstop)[colnames(busstop) == "BUS_STOP_N"] <- "ORIGIN_PT_CODE"
```

We will also import the layout of Singapore for excluding bus stops that are not found in Singapore.

```{r}
#| code-fold: false
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)

```

After that we will create the hexagons that will create the map layout. All of the hexagons will also be given a grid id name that can be used for identifying each individual grid.

```{r}
#| code-fold: false
area_honeycomb_grid <- st_make_grid(busstop, cellsize = 750, what = "polygons", square = FALSE)
honeycomb_grid_sf <- st_sf(area_honeycomb_grid) %>%
  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))

```

# Data processing

## Assigning individual bus stop to hexagons

First we will assign the bus stop point geometry data to each polygon using *st_intersection()* of Singapore to obtain bus stops located locally followed by another intersection with the hexagon map. The function assigns all of the points to a polygon by the point-set intersection of two geometries. Additional information [here](https://postgis.net/docs/ST_Intersection.html).

```{r}
#| code-fold: false
valid_busstop <- st_intersection(busstop, mpsz)
busstop_hex <- st_intersection(valid_busstop, honeycomb_grid_sf) %>%
  select(1,10)%>%
  st_drop_geometry()
busstop_hex <- unique(busstop_hex)

```

## Duplication check

We will only keep data points that are unique using the *unique()* function. This function will only keep rows that are unique in the data set.

```{r}
#| code-fold: false
unique_weekdayPM <- unique(weekdayPM)
```

## Assign grid id to origin and destination

Using the grid that we have created in the beginning, we will now join the data that contains the destination together with the grid. Our busstop_hex variable contains a column with the bus stop number and another column with the grid id. Since we have renamed our bus stop number column name as the ORIGIN_PT_CODE, which can also be found in our trips data set, they can now be joined using that as a common point, hence assigning the grid id to the origin bus stops. The newly assigned grid id will be renamed to avoid confusion when joining the grid id again for the destination bus stop next.

```{r}
#| code-fold: false
combn_des_weekdayPM <- left_join(unique_weekdayPM,busstop_hex) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_GRID = grid_id,
         DESTIN_BS = DESTINATION_PT_CODE)
unique(combn_des_weekdayPM)
```

::: callout-note
### Caution

Using left_join() will join the right data set to the left, so be careful to avoid losing wanted data. Use right_join() or full_join() as needed. More information [here](https://dplyr.tidyverse.org/reference/mutate-joins.html).
:::

Here we will join the destination grid ID by matching the destination bus stop code and the bus stop code so as to match the destination bus stop to their grid. Aside from joining them using a common column name, we can also force this pairing using a **by = ()** argument. After that we will check for duplications again.

```{r}
#| code-fold: false
OD_weekdayPM <- left_join(combn_des_weekdayPM , busstop_hex,
            by = c("DESTIN_BS" = "ORIGIN_PT_CODE"))
OD_weekdayPM <- unique(OD_weekdayPM)

```

To make it clearer we will rename the **grid_ID** column, which is the destination grid id into **DESTIN_GRID**. We will also sum up all of the trips made that have a unique origin and destination pair. This way we can calculate the total trips that are taken using this particular route.

```{r}
#| code-fold: false
OD_weekdayPM <- OD_weekdayPM %>%
  rename(DESTIN_GRID = grid_id)%>%
  drop_na()%>%
  group_by(ORIGIN_GRID, DESTIN_GRID) %>%
  summarise(TRIPS = sum(TRIPS))


```

We will have to remove any intra hexagonal trips.

```{r}
#| code-fold: false
OD_weekdayPM <- OD_weekdayPM[OD_weekdayPM$ORIGIN_GRID!=OD_weekdayPM$DESTIN_GRID,]

```

## Creating desire lines

Here we will create the desire lines from the different origin grids and destination grids using the [*od2line*](https://www.rdocumentation.org/packages/stplanr/versions/0.1.1/topics/od2line)*()* function.

```{r}
#| code-fold: false
OD_weekdayPM <- od2line(flow = OD_weekdayPM, 
                    zones = honeycomb_grid_sf,
                    zone_code = "grid_id")

```

We will recreate the hexagonal-polygon data so that we are able to map out the hexagons onto the Singapore map. This is only for mapping and is different from the earlier hexagonal maps as we want the geometry data of each hexagon and their grid id.

```{R}
#| code-fold: false
valid_hex <- st_intersection(valid_busstop, honeycomb_grid_sf)%>%
  select(1,10) %>%
  st_drop_geometry()
valid_hex <- left_join(honeycomb_grid_sf, valid_hex) %>%
  drop_na()
```

## Visualisation of trips

Here we can visualise the flow of passengers between grids using the desire lines that we have created. We will look at all of the passenger flow, the top 5%, the bottom 5% and the interquartile range of the number of trips.

::: panel-tabset
### All trips

```{r}

tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +
  tm_polygons()+
OD_weekdayPM %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Passenger flow of the all trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

As we can see, the number of trips can be too numerous and we can make it clearer by limiting the plot to only a certain number of trips.

### Top 5%

Before we plot, we will have to calculate the smallest number of trips that makes it to the top 5% of trips.

```{r}
#| code-fold: false
top5 <- OD_weekdayPM[OD_weekdayPM$TRIPS >= quantile(OD_weekdayPM$TRIPS, probs = 1-5/100),]
top5trips <- min(top5$TRIPS)
```

We can visualise the flow of bus passengers of the top 5% ridership count from each origin grid to the destination grid.

```{r}
tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +
  tm_polygons()+
OD_weekdayPM %>%  
  filter(TRIPS >= top5trips) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5) +
  tm_layout(main.title = "Passenger flow of the top 5% of trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

### Bottom 5% of trips

Here we will find out the maximum trip count for the bottom 5% of all trips made.

```{r}
bot5 <- OD_weekdayPM[OD_weekdayPM$TRIPS <= quantile(OD_weekdayPM$TRIPS, probs = 5/100),]
bot5trips <- max(bot5$TRIPS)

```

```{r}

tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +
  tm_polygons()+
OD_weekdayPM %>%  
  filter(TRIPS <= bot5trips) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Passenger flow of the bottom 5% of trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

We can still see that there are too many desire lines to make out proper data. This means that even though we only extracted the bottom 5% of all trips, most of the trips are made with only 1 trip count.

### Mean trips

We can calculate where half or more trips are made by calculating the mean number of trips.

```{r}
#| code-fold: false
summary(OD_weekdayPM$TRIPS)

```

We can see that mean is 343.4 passengers made the same trip from a origin-destination grid pair.

```{r}
tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +
  tm_polygons()+
OD_weekdayPM %>%  
  filter(TRIPS >= 343.4) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)+
  tm_layout(main.title = "Half or more passenger flow of trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

We see that the data cleared up a lot when compared against the bottom 5% of all trips made and is closer to the top 5% of trips made. When combined with the median result of 35, the average passenger count of 343.4 means more trips are made with a low number of passengers and there are popular routes that are often taken in the early morning peak period. The large number of passengers on certain popular routes causes the data to skew.

### Top 10 routes

Aside from using the percentage, we can also plot out the top 10 most popular routes made during weekday morning peak period. We will first obtain the routes with top 10 trip count.

```{r}
#| code-fold: false
top10 <- head(arrange(OD_weekdayPM, desc(TRIPS)), n = 10)
top10trips <- min(top10$TRIPS)

```

```{r}
tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +

  tm_polygons()+
OD_weekdayPM %>%  
  filter(TRIPS >= top10trips) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 10,
           alpha = 0.5)+
  tm_layout(main.title = "Top 10 most popular routes of trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

We can see that for most popular bus routes, the distance between the origin and destination grids is relatively short. Longer distance travel may be more efficient via other transport modes such as trains.

We can also take a look at the bus stops that are found within the most popular origin-destination grid pair.

```{r}
top_df <- head(arrange(OD_weekdayPM, desc(TRIPS)), n = 1) %>% 
  st_drop_geometry()
busstop_hex_df <- st_drop_geometry(busstop_hex)

toporigin <- top_df %>%
  left_join (busstop_hex_df,
             by = c("ORIGIN_GRID" = "grid_id")) %>%
  rename(origin_bs = ORIGIN_PT_CODE)

toporigin_des <- toporigin %>%
  left_join (busstop_hex_df,
             by = c("DESTIN_GRID" = "grid_id")) %>%
  rename(des_bs = ORIGIN_PT_CODE)

toporigin_bs <- data.frame(unique(toporigin_des$origin_bs)) %>% 
  rename(ORIGIN_PT_CODE = 1)

topdes_bs <- data.frame(unique(toporigin_des$des_bs))%>% 
  rename(ORIGIN_PT_CODE = 1)

topori_bsname <- left_join(toporigin_bs, valid_busstop)
topdes_bsname <- left_join(topdes_bs, valid_busstop)

ori <- toString(topori_bsname$LOC_DESC)
des <- toString(topdes_bsname$LOC_DESC)



mostpop <- data.frame((ori), (des)) %>%
  rename("Most popular origin bustops" = 1,
         "Most popular destination bustops" = 2)


datatable(mostpop)
```

We can see the bus stops that are found within the most popular origin-destination grid pair. Authorities can utilise the above information to plan for more buses to cater to those routes so as to ease traffic if needed.
:::

# Distance Matrix

Here we will begin calculating the distance between each hexagon and the other hexagons that contains bus stops. Normally people are less likely to take trips if the distance is longer.

We will create a hex data containing the grid id and the polygon information. Essentially this has the same information as the valid_hex variable that we have created earlier. But here we will add an additional step to filter out any duplicates.

```{r}
hex <- left_join( honeycomb_grid_sf,busstop_hex)%>%
  group_by(grid_id)%>%
  drop_na()%>%
  select(1)
hex<-unique(hex)

```

Next we will convert it to a *Spatial Polygonal Dataframe* object using the *as()* function.

```{r}
#| code-fold: false
hex_sp <- as(hex, "Spatial")
hex_sp
```

Next we will calculate the distance between the grids. The [*spDists*](https://www.rdocumentation.org/packages/sp/versions/2.1-1/topics/spDistsN1)*()* function will use the center of each hexagon for calculating the distance between grids. This way we are able to standardise the reference point of each grid to the other grids.

```{r}
#| code-fold: false
DIST <- spDists(hex_sp, 
                longlat = FALSE)
head(DIST, n=c(10, 10))
```

We will then rename the column and row names with their respective grid ids.

```{r}
#| code-fold: false
sz_names <- hex$grid_id

colnames(DIST) <- paste0(sz_names)
rownames(DIST) <- paste0(sz_names)
```

We can then pivot the dataframe to form a table for easier referencing.

```{r}
distPair <- melt(DIST) %>%
  rename(DIST = value)
head(distPair, 10)

```

Here we will substitute any distance of 0 with 50m as we will log our subsequent distance data, this will avoid log(0) errors.

```{R}
distPair %>%
  filter(DIST > 0) %>%
  summary()
distPair$DIST <- ifelse(distPair$DIST == 0,
                        50, distPair$DIST)
distPair %>%
  summary()
```

We can then rename the column names.

```{R}
distPair <- distPair %>%
  rename(orig = Var1,
         dest = Var2)
```

We will reassign the variable name for subsequent processing. We do this so as to avoid manipulating data that we do not want to modify.

```{R}
flow_data <- OD_weekdayPM
```

Here we will join the distance between grids and their respective origin-destination grid pairs. This will allow us to add the distances to the each origin-destination pair with the total number of trips made by that pair of grids.

```{R}
flow_data1 <- flow_data %>%
  left_join (distPair,
             by = c("ORIGIN_GRID" = "orig",
                    "DESTIN_GRID" = "dest"))
```

## Plotting

```{R}
ggplot(data = flow_data1,
       aes(x = log(DIST),
           y = log(TRIPS))) +
  geom_point() +
  geom_smooth(method = lm)

```

We see that the graph shows an inverse relationship between trips and distance. This is expected as the further a place is, the less likely people would want to travel.

## Attractive and propulsive factor processing

Here we will prepare the data into attractive and propulsive sections. To differentiate that we will use \_A for each variable and \_P for variables used for attractive and propulsive respectively.

```{R}
#| code-fold: false
hex_A <- hex
hex_P <- hex
```

::: panel-tabset
## **Attractive**

Here we will assemble the attractive factors, which are the number of estimated HDB households, retail stores and MRT exits in each hexagon.

We will also be using the number of dwelling units as an estimation for the number of households staying in that HDB.

```{r}
#| code-fold: false
dwell_sf <- hdb_sf %>%
  select(13) 
datatable(dwell_sf)
```

Next we will join the number of households that belong to a grid with their respective grids.

```{R}
#| code-fold: false
hex_hdb<- st_intersection(hex_A, dwell_sf)%>%
  group_by(grid_id) %>%
  summarise(total_dwelling_units = sum(total_dwelling_units))%>%
  rename(DWELL_COUNT = total_dwelling_units) %>%
  st_drop_geometry()

hex_A <- left_join(hex_A, hex_hdb) %>% 
  mutate(DWELL_COUNT = ifelse(is.na(DWELL_COUNT), 0, DWELL_COUNT))
```

Other information that we will include in our attractive factors include the number of retail outlets and MRT exits.

```{r}
#| code-fold: false
hex_A$retail_COUNT <- lengths(st_intersects(hex_A, retail_sf))  
hex_A$stn_exit_COUNT <- lengths(st_intersects(hex_A, stn_exit_sf))
```

::: panel-tabset
Here we will see if the top 100 most popular bus routes will intersect with the our attractive factors. Firstly we will obtain our top 100 most popular bus routes.

```{R}
top100 <- head(arrange(OD_weekdayPM, desc(TRIPS)), n = 100)
```

### HDB and trips made

We can look at the number of trips made to grids that contains HDBs. To do that we can first create grids with that contains HDBs. We will then check if the destination grid of our top 100 most popular trips have commonality with our grids that contains schools using *left_join()*.

```{r}
hdb_grid <- hex_A%>%
  filter(DWELL_COUNT >0) %>%
  select(1,3) %>%
  rename(DESTIN_GRID = grid_id)%>%
  st_drop_geometry()

hdb_des <- left_join(top100, hdb_grid) %>%
  drop_na()

```

```{r}
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz)+  
  tm_polygons()+
  tm_shape(hex_A)+  
  
  tm_fill("DWELL_COUNT", 
          style = "pretty",
          palette = "Blues",
          title = "number of households") +
  tm_polygons()+
  
  
  tm_shape(hdb_sf)+ 
  tm_dots(alpha = 0.05)+
  
hdb_des %>%  

tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0,1,3,5,10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Common destination grid of top 100 trips and HDBs(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)



```

We see a clusters of residential areas across Singapore. We can also see that most of the popular bus routes contains HDBs as their destination, this may mean that residential areas serves as a point of attraction since people are going back home in the evening.

### Retail count and trips made

```{r}
retail_grid <- hex_A %>%
  filter(retail_COUNT >0)%>%
  select(1,4) %>%
  rename(DESTIN_GRID = grid_id)%>%
  st_drop_geometry()

retail_des <- left_join(top100, retail_grid) %>%
  drop_na()
summary(retail_des$retail_COUNT)


```

```{r}
tmap_options(check.and.fix = TRUE)  
tm_shape(mpsz)+  #sg boundary
  tm_polygons()+
  tm_shape(hex_A)+  
  
  tm_fill("retail_COUNT", 
          style = "pretty", 
          palette = "Reds",
          title = "number of retail outlets") +
  tm_polygons()+
  
  
  tm_shape(retail_sf)+
  tm_dots(alpha = 0.5)+
  
retail_des %>%  

tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0,1,3,5,10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Common destination grid of top 100 trips and retails(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)




```

From this data we can see that a retail outlets are generally quite widespread but can also be seen to cluster at the heartlands. Perhaps due to the widespread locations of retail outlets, we can see high commonality between the popular bus routes and retail locations.

### MRT exits and trips made

```{r}

mrt_grid <- hex_A %>%
  filter(stn_exit_COUNT >0)%>%
  select(1,5) %>%
  rename(DESTIN_GRID = grid_id)%>%
  st_drop_geometry()

mrt_des <- left_join(top100, mrt_grid) %>%
  drop_na()


```

```{r}

tmap_options(check.and.fix = TRUE)  
tm_shape(mpsz)+  #sg boundary
  tm_polygons()+
  tm_shape(hex_A)+ 
  
  tm_fill("stn_exit_COUNT", 
          style = "pretty",
          palette = "Greens",
          title = "number of MRT exits") +
  tm_polygons()+

    tm_shape(fin_sf)+  
  tm_dots(alpha = 0.1)+
  
mrt_des %>%  
 tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0,1,3,5,10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Common destination grid of top 100 trips and MRT exits(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)



```

We can once again see that the heartlands contains the most number of MRT exits, this is likely from the number of larger train stations which services multiple lines, hence requiring more exits to prevent bottle-necking during rush hour. We see some commonality between train stations exits and our popular bus stop destinations.
:::

## **Propulsive**

We will match all of the number of schools, businesses and financial services to their respective grids.

```{R}
#| code-fold: false
hex_P$SCHOOL_COUNT <- lengths(st_intersects(hex_P, schools_sf))
hex_P$BIZ_COUNT <- lengths(st_intersects(hex_P, biz_sf))
hex_P$FIN_COUNT <- lengths(st_intersects(hex_P, fin_sf))


```

```{r}
tmap_options(check.and.fix = TRUE)  
tm_shape(mpsz)+  #sg boundary
  tm_polygons()+
  tm_shape(hex_P)+ 
  tm_polygons()+
  

    tm_shape(schools_sf)+  
  tm_dots(col = "blue", alpha = 0.5)+
  tm_shape(biz_sf)+  
  tm_dots(col = "red", alpha = 0.3)+
  tm_shape(fin_sf)+  
  tm_dots(col = "green", alpha = 0.1)+
  

  tm_layout(main.title = "Schools, businesses and mrt exits distribution",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

To avoid over concentrating this page with maps, we will combine the distribution of schools, businesses and financial services together. We can see the distribution of schools, businesses and financial services dotted in blue, red and green respectively. We can see concentration of businesses in the industrialised zones and businesses in the heartland regions.
:::

# Model calibration

## Flow data preparation

In this section we will prepare our data for subsequent model calibration.

Since the hex variable still contains geometry information, which will prevent us from joining the various school and business information with ridership using **left_join**, we will drop it.

```{r}
hex_A <- st_drop_geometry(hex_A)
hex_P <- st_drop_geometry(hex_P)

```

We can then join the school and business count information to the ridership using left_join. For attractive factors, we will join it by destination grid since the point of attractions are located in the destination grid. For propulsive factors, we will use origin grid since they are factors that will push us from the origin.

```{r}
#| code-fold: false
flow<- flow_data1 %>%
  left_join (hex_A,
             by = c("DESTIN_GRID" = "grid_id"))

flow<- flow %>%
  left_join (hex_P,
             by = c("ORIGIN_GRID" = "grid_id"))
```

Since we will log the data for modelling, we will have to convert any 0 or NA value data amongst the different factors into a positive number that is less than 1. We will use 0.99.

```{r}


flow$DWELL_COUNT <- ifelse(
  is.na(flow$DWELL_COUNT) | flow$DWELL_COUNT == 0,
  0.99, flow$DWELL_COUNT)

flow$stn_exit_COUNT <- ifelse(
  is.na(flow$stn_exit_COUNT) | flow$stn_exit_COUNT == 0,
  0.99, flow$stn_exit_COUNT)

flow$retail_COUNT <- ifelse(
  is.na(flow$retail_COUNT) | flow$retail_COUNT == 0,
  0.99, flow$retail_COUNT)


flow$BIZ_COUNT <- ifelse(
  is.na(flow$BIZ_COUNT) | flow$BIZ_COUNT == 0,
  0.99, flow$BIZ_COUNT)

flow$SCHOOL_COUNT <- ifelse(
  is.na(flow$SCHOOL_COUNT) | flow$SCHOOL_COUNT == 0,
  0.99, flow$SCHOOL_COUNT)

flow$FIN_COUNT <- ifelse(
  is.na(flow$FIN_COUNT) | flow$FIN_COUNT == 0,
  0.99, flow$FIN_COUNT)

```

We will also have to convert the grid names of the origin and destination into characters. We have been using numerals for naming them which would be recognised as integers by R.

```{r}
#| code-fold: false
flow$ORIGIN_GRID <- as.character(flow$ORIGIN_GRID)
flow$DESTIN_GRID <- as.character(flow$DESTIN_GRID)


```

```{r}
flow$FlowNoIntra <- ifelse(
  flow$ORIGIN_GRID == flow$DESTIN_GRID, 0, flow$TRIPS)
flow$offset <- ifelse(
  flow$ORIGIN_GRID == flow$DESTIN_GRID, 0.000001, 1)

inter_zonal_flow <- flow %>%
  filter(FlowNoIntra > 0)

```

To make sure we are able to capture all of our output, we will adjust the maximum print length into a large integer.

```{r}
#| code-fold: false
options(max.print = 100000)

```

## Goodness-of-fit

Goodness-of-fit refers to how well sample data is able to fit the curve. To calculate that we will first have to create our own R-squared function.

```{r}
#| code-fold: false
CalcRSquared <- function(observed, estimated){
  r<-cor(observed, estimated)
  R2<-r^2
  R2}

```

## Modelling calibration

Here we will begin our actual modelling for our different factors.

The models that will be used today are known as spatial interaction models where flows between spatial entities are estimated. The models that we will be using are:

-   Origin constrained model
-   Destination constrained model
-   Unconstrained model
-   Doubly constrained model

The modelling will be done by using the *glm()* function from the **stats** package. It allows us to fit a generalised linear model, more information can be found [here](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm).

::: panel-tabset

### Origin(Production) constrained model(Attractive modelling)

```{R}


orcSIM_Poisson <- glm(formula = TRIPS ~
                        ORIGIN_GRID + 
                          
                        log(DWELL_COUNT)+
                        log(retail_COUNT) +
                        log(stn_exit_COUNT)+
                        
                        log(DIST) - 1, 
                      family = poisson(link = "log"),
                      data = inter_zonal_flow,
                      na.action = na.exclude)

summary(orcSIM_Poisson)

```

We can see that for all of the attractive attributes showing good attractive pull for trips towards there.

We can use the *summary()* function to obtain various coefficients and statistical analysis from our origin model as well as other models. ::: callout-note \#### Attention

Here we will only use factors that are attractive for origin constrained and there is a -1 to the distance in order to remove the intercept which would otherwise be inserted.


```{r}
#| code-fold: false
CalcRSquared(orcSIM_Poisson$data$TRIPS, orcSIM_Poisson$fitted.values)

```

We have a R-square value of 0.432, which indicates that about 43% of variation can be explained by the model.

### Destination constrained(Propulsive modelling)

```{R}

decSIM <- glm(formula = TRIPS ~ 
                DESTIN_GRID + 
                log(SCHOOL_COUNT) +
                log(BIZ_COUNT)+
                log(FIN_COUNT)+
                log(DIST),
              family = poisson(link = "log"),
              data = inter_zonal_flow,
              na.action = na.exclude)
summary(decSIM)



```

```{r}
#| code-fold: false
CalcRSquared(decSIM$data$TRIPS, decSIM$fitted.values)

```

We have a R-square value of 0.356, which indicates that about 36% of variation can be explained by the destination constrained model.

::: callout-note
#### Attention

Here we will only use factors that are propulsive for destination constrained models.
:::

### Common modelling sets

::: panel-tabset
#### Unconstrained modelling

```{R}

uncSIM <- glm(formula = TRIPS ~ 
                  log(SCHOOL_COUNT) +
                log(BIZ_COUNT)+
                log(FIN_COUNT)+
               
                        log(DWELL_COUNT)+
                        log(retail_COUNT) +
                        log(stn_exit_COUNT)+
                log(DIST),
              family = poisson(link = "log"),
              data = inter_zonal_flow,
              na.action = na.exclude)

uncSIM

```

```{r}
#| code-fold: false
CalcRSquared(uncSIM$data$TRIPS, uncSIM$fitted.values)

```

We have a R-square value of 0.223, which indicates that only about 22.3% of variation can be explained by the unconstrained model.

#### Doubly Constrained modelling

```{r}

dbcSIM_Poisson <- glm(formula = TRIPS~
                        ORIGIN_GRID + 
                        DESTIN_GRID + 
                        log(DIST), 
                      family = poisson(link = "log"),
                      data = inter_zonal_flow,
                      na.action = na.exclude)
summary(dbcSIM_Poisson)

```

```{r}
#| code-fold: false
CalcRSquared(dbcSIM_Poisson$data$TRIPS, dbcSIM_Poisson$fitted.values)

```

Here we have a R-square value of 0.586, which indicates that about 59% of variation can be explained by the doubly model, the best so far.
:::

:::

# Model Comparison

To compare the suitability between different models, we can use the Root-mean-square deviation(RMSE). We can do that using **performance** package.

We will first list all of the models used for attractive modelling.

```{r}

model_list <- list(unconstrained=uncSIM,
                   originConstrained=orcSIM_Poisson,
                   destinationConstrained=decSIM,
                  
                   doublyConstrained=dbcSIM_Poisson)
```

We can then compare using the *compare_performance()* function of the **performance** package.

```{r}
#| code-fold: false
compare_performance(model_list,
                    metrics = "RMSE")

```

From the different RMSE calculated above, we can see that the doubly constrained model has the lowest RMSE value of 1043.379, which represents the most accurate model out of the tested models.

## Plotting

Here we can visualise the modelling of the graphs between the observed and fitted values. Before we do that we will sort out fitted values into a dataframe.

::: panel-tabset
### Origin constrained

```{r}
#| code-fold: false
df_model <- as.data.frame(orcSIM_Poisson$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow <- inter_zonal_flow %>%
  cbind(df_model)
```

### Destination constrained

```{r}
#| code-fold: false
df_model <- as.data.frame(decSIM$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow <- inter_zonal_flow %>%
  cbind(df_model) 
```

### Unconstrained

```{r}
#| code-fold: false
df_model <- as.data.frame(uncSIM$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow <- inter_zonal_flow %>%
  cbind(df_model)
```

### Doubly constrained

```{r}
#| code-fold: false
df_model <- as.data.frame(dbcSIM_Poisson$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow <- inter_zonal_flow %>%
  cbind(df_model) 
```
:::

Here we will rename the columns for easier recognition.

```{r}
inter_zonal_flow <- inter_zonal_flow %>%
  rename(uncTRIPS = "uncSIM.fitted.values",
         decTRIPS = "decSIM.fitted.values",
         orcTRIPS = "orcSIM_Poisson.fitted.values",
         dbcTRIPS = "dbcSIM_Poisson.fitted.values")

```



Finally we can then plot the scatterplots using *geom_point()* from the **ggplot** package.



```{r}

unc_p <- ggplot(data = inter_zonal_flow,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dec_p <- ggplot(data = inter_zonal_flow,
                aes(x = decTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

orc_p <- ggplot(data = inter_zonal_flow,
                aes(x = orcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)



dbc_p <- ggplot(data = inter_zonal_flow,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

ggarrange(unc_p, orc_p,dec_p, dbc_p,
          ncol = 2,
          nrow = 2)
```

The plot shows the the observed(y-axis) and an estimated value(x-axis). Each observed and estimated pair trip value will form a dot on the graph. We can see that amongst the 4 plots, the plot from the doubly constrained model shows the best grouping, indicative of the accuracy from that model as seen previously. This is also echoed from its high R-square value and low RMSE value. The unconstrained model shows the widest spread.

