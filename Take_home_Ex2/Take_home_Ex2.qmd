---
title: "Take home exercise 2"
---

---
title: "Take-home_Ex2"
author: "Mah Lian Khye"
date: "14 December 2023"
date-modified: "last-modified"

format: 
  html:
    self_contained: false
    code-fold: true
    code-summary: "code chunk"
execute:
  echo: true # all code chunks will appear
  eval: true # all code chunks will run live (be evaluated)
---

# Overview

Organising bodies will have to consider passenger usage to and from locations when adding, changing or removing current transportation routes. Any alterations to present routes can lead to passengers having to seek alternate travelling options that can impact travel time and distance. To minimise the impact, the travelling routes taken will have to be surveyed thorugh commuters survey. However this form of sampling can be inefficient and will easily be out of date. The use of passenger tracking methods such as GPS or smart card boarding and alighting locations may be used as a substitute that can generate data more readily.

# Objective

The main objectives of this exercise is to analyse the boarding and alighting locations of passengers of a time interval. This analysis will allow us to track the factors that affect urban mobility so that route usage may be analyse for future route planning.

# Tasks

The tasks of this exercise are as follow: - Derive an analytical hexagon map to be used as a traffic analysis zone - Construct an O-D matrix of commuter flow for weekday mornings from 6am to 9am - Display the O-D flows - Assemble at least 3 propulsive and 3 attractiveness variables - Compute a distance matrix - Calibrate spatial interactive models and present the modelling results using geovisualisation and graphical visualisation methods

# Importing packages and data

::: panel-tabset
## Import packages

Firstly, we will import the various packages that we will need for this exercise.

```{r}
#| code-fold: false
pacman::p_load(tmap, sf, sp, DT, 
               performance, reshape2,
               tidyverse, dplyr,  stplanr, ggpubr)
```

::: callout-note
### Package functions

-   **tmap**: choropleth map visualisation
-   **sf**: package containing functions to support simple features and standardised a way to encode spatial vector data
-   **DT**: for creating DataTables
-   **performance**: for assessing model quality
-   **reshape2**: for reshaping data
-   **tidyverse**: for manipulating and data presentations
-   **dplyr**: for working with dataframes
-   **stplanr**: for creating mobility lines
-   **ggpubr**: for manipulating plots
:::

## Import data

Next we will import the data to be used for analysis.

::: panel-tabset
### **Loading aspatial table**

Here we will import the ridership of the different bus stops in Oct 2023.

```{r}
#| code-fold: false
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```

Next we will extract the ridership for weekdays from 5pm to 8pm only.

```{r}
#| code-fold: false
weekdayPM <- odbus %>% 
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 & TIME_PER_HOUR <= 20) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

```

### Loading geospatial data

Next we will import all of the bus stops and their coordinates and attached it to the *busstop* variable.

```{r}
#| code-fold: false
busstop <- st_read(dsn = "data/geospatial", layer = "BusStop") %>%
  st_transform(crs = 3414)
```

We will also import all of the attractive and propulsive factors that we will use for subsequent modelling analysis.

::: panel-tabset
#### Attractive factors

For attractive factors we will be using HDBs, retail outlets and train station exits as people either take buses to return home directly or to the train stations for their subsequent journey after work. There might be people who choose to do shopping after work or dinner, therefore retail stores have been chosen as a attractive factor. Here we will import the aspatial data for them.

For HDB data we will have to process them from latitude and longitude points to projected coordinate system.

```{r}
#| code-fold: false
hdb <- read_csv("data/aspatial/hdb.csv")
hdb_sf <- st_as_sf(hdb, coords = c("lng", "lat"), crs = st_crs(4326)) %>%
  st_transform(crs = 3414)

```

We can import the retail stores and train station exits data directly as they are already in the correct coordinate system that we will be using in this exercise.

```{r}
#| code-fold: false

retail_sf <- st_read(dsn = "data/geospatial", 
                layer = "Retails") %>%
  st_transform(crs=3414)


stn_exit_sf <- st_read(dsn = "data/geospatial", 
                layer = "Train_Station_Exit_Layer") %>%
  st_transform(crs=3414)
```

::: callout-note
### Coordinate systems

Remember to use the correct coordinate system so that datas can be matched correctly! More information for the system used can be found [here](https://epsg.io/3414).
:::

#### Propulsive factors

For school data we will have to process them from latitude and longitude points to projected coordinate system.

```{R}
schools <- read_csv("data/aspatial/schools.csv")
schools <- schools %>%
  rename("latitude" = "results.LATITUDE",
         "longitude" = "results.LONGITUDE") %>%
  select(postal_code, school_name, latitude, longitude)
schools_sf <- st_as_sf(schools, coords = c("longitude", "latitude"), crs = st_crs(4326)) %>%
  st_transform(crs = 3414)
```

For financial services such as banks and businesses, we can import the geospatial data directly.

```{r}

fin_sf <- st_read(dsn = "data/geospatial", 
                layer = "FinServ") %>%
  st_transform(crs=3414)
biz_sf <- st_read(dsn = "data/geospatial", 
                layer = "Business") %>%
  st_transform(crs=3414)

```
:::
:::
:::



We will first rename the bus stop column title for easier data joining.

```{r}
colnames(busstop)[colnames(busstop) == "BUS_STOP_N"] <- "ORIGIN_PT_CODE"
```

We will also import the layout of Singapore for excluding bus stops that are not found in Singapore.

```{r}
#| code-fold: false
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)

```

After that we will create the hexagons that will create the map layout. All of the hexagons will also be given a grid id name that can be used for identifying each individual grid.

```{r}
#| code-fold: false
area_honeycomb_grid <- st_make_grid(busstop, cellsize = 750, what = "polygons", square = FALSE)
honeycomb_grid_sf <- st_sf(area_honeycomb_grid) %>%
  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))

```

# Data processing

## Assigning individual bus stop to hexagons

First we will assign the bus stop point geometry data to each polygon using *st_intersection()* of Singapore to obtain bus stops located locally followed by another intersection with the hexagon map. The function assigns all of the points to a polygon by the point-set intersection of two geometries. Additional information [here](https://postgis.net/docs/ST_Intersection.html).

```{r}
#| code-fold: false
valid_busstop <- st_intersection(busstop, mpsz)
busstop_hex <- st_intersection(valid_busstop, honeycomb_grid_sf) %>%
  select(1,10)%>%
  st_drop_geometry()
busstop_hex <- unique(busstop_hex)

```

## Duplication check

We will only keep data points that are unique using the *unique()* function.

```{r}
#| code-fold: false
unique_weekdayPM <- unique(weekdayPM)
```

## Joining destination table and grid

Using the grid that we have created in the beginning, we will now join the data that contains the destination together with the grid.

```{r}
#| code-fold: false
combn_des_weekdayPM <- left_join(unique_weekdayPM,busstop_hex) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_GRID = grid_id,
         DESTIN_BS = DESTINATION_PT_CODE)
unique(combn_des_weekdayPM)
```

Here we will join the destination grid ID by matching the destination bus stop code and the bus stop code so as to match the destination bus stop to their grid. After that we will check for duplications again.

```{r}
#| code-fold: false
OD_weekdayPM <- left_join(combn_des_weekdayPM , busstop_hex,
            by = c("DESTIN_BS" = "ORIGIN_PT_CODE"))
OD_weekdayPM <- unique(OD_weekdayPM)

```

To make it clearer we will rename the **grid_ID** column, which is the destination grid id into **DESTIN_GRID**. We will also sum up all of the trips made that have a unique origin and destination pair. This way we can calculate the trips that are taken using this particular route.

```{r}
#| code-fold: false
OD_weekdayPM <- OD_weekdayPM %>%
  rename(DESTIN_GRID = grid_id)%>%
  drop_na()%>%
  group_by(ORIGIN_GRID, DESTIN_GRID) %>%
  summarise(TRIPS = sum(TRIPS))


```

We will have to remove any intra hexagonal trips.

```{r}
#| code-fold: false
OD_weekdayPM <- OD_weekdayPM[OD_weekdayPM$ORIGIN_GRID!=OD_weekdayPM$DESTIN_GRID,]

```

Here we will create the desire lines from the different origin grids and destination grids.

```{r}
#| code-fold: false
OD_weekdayPM <- od2line(flow = OD_weekdayPM, 
                    zones = honeycomb_grid_sf,
                    zone_code = "grid_id")

```

We will recreate the hexagonal-polygon data so that we are able to map out the hexagons onto the Singapore map. This is only for mapping and is different from the earlier hexagonal maps as we want the geometry data of each hexagon and their grid id.

```{R}
#| code-fold: false
valid_hex <- st_intersection(valid_busstop, honeycomb_grid_sf)%>%
  select(1,10) %>%
  st_drop_geometry()
valid_hex <- left_join(honeycomb_grid_sf, valid_hex) %>%
  drop_na()
```

## Visualisation of trips

Here we can visualise the flow of passengers between grids using the desire lines that we have created. We will look at all of the passenger flow, the top 5%, the bottom 5% and the interquartile range of the number of trips.

::: panel-tabset
### All trips

```{r}

tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +
  tm_polygons()+
OD_weekdayPM %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Passenger flow of the all trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

As we can see, the number of trips can be too numerous and we can make it clearer by limiting the plot to only a certain number of trips.

### Top 5%

Before we plot, we will have to calculate the smallest number of trips that makes it to the top 5% of trips.

```{r}
#| code-fold: false
top5 <- OD_weekdayPM[OD_weekdayPM$TRIPS >= quantile(OD_weekdayPM$TRIPS, probs = 1-5/100),]
top5trips <- min(top5$TRIPS)
```

We can visualise the flow of bus passengers of the top 5% ridership count from each origin grid to the destination grid.

```{r}
tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +
  tm_polygons()+
OD_weekdayPM %>%  
  filter(TRIPS >= top5trips) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5) +
  tm_layout(main.title = "Passenger flow of the top 5% of trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

### Bottom 5% of trips

Here we will find out the maximum trip count for the bottom 5% of all trips made.

```{r}
bot5 <- OD_weekdayPM[OD_weekdayPM$TRIPS <= quantile(OD_weekdayPM$TRIPS, probs = 5/100),]
bot5trips <- max(bot5$TRIPS)

```

```{r}

tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +
  tm_polygons()+
OD_weekdayPM %>%  
  filter(TRIPS <= bot5trips) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Passenger flow of the bottom 5% of trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

We can still see that there are too many desire lines to make out proper data. This means that even though we only extracted the bottom 5% of all trips, most of the trips are made with only 1 trip count.

### Mean trips

We can calculate where half or more trips are made by calculating the mean number of trips.

```{r}
#| code-fold: false
summary(OD_weekdayPM$TRIPS)

```

We can see that mean is 343.4 passengers made the same trip from a origin-destination grid pair.

```{r}
tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +
  tm_polygons()+
OD_weekdayPM %>%  
  filter(TRIPS >= 343.4) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)+
  tm_layout(main.title = "Half or more passenger flow of trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

We see that the data cleared up a lot when compared against the bottom 5% of all trips made and is closer to the top 5% of trips made. When combined with the median result of 35, the average passenger count of 343.4 means more trips are made with a low number of passengers and there are popular routes that are often taken in the early morning peak period. The large number of passengers on certain popular routes causes the data to skew.

### Top 10 routes

Aside from using the percentage, we can also plot out the top 10 most popular routes made during weekday morning peak period. We will first obtain the routes with top 10 trip count.

```{r}
#| code-fold: false
top10 <- head(arrange(OD_weekdayPM, desc(TRIPS)), n = 10)
top10trips <- min(top10$TRIPS)

```

```{r}
tm_shape(mpsz) +
  tm_polygons() +
  tm_shape(valid_hex) +

  tm_polygons()+
OD_weekdayPM %>%  
  filter(TRIPS >= top10trips) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 10,
           alpha = 0.5)+
  tm_layout(main.title = "Top 10 most popular routes of trip count made(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

```

We can see that for most popular bus routes, the distance between the origin and destination grids is relatively short. Longer distance travel may be more efficient via other transport modes such as trains.

We can also take a look at the bus stops that are found within the most popular origin-destination grid pair.

```{r}
top_df <- head(arrange(OD_weekdayPM, desc(TRIPS)), n = 1) %>% 
  st_drop_geometry()
busstop_hex_df <- st_drop_geometry(busstop_hex)

toporigin <- top_df %>%
  left_join (busstop_hex_df,
             by = c("ORIGIN_GRID" = "grid_id")) %>%
  rename(origin_bs = ORIGIN_PT_CODE)

toporigin_des <- toporigin %>%
  left_join (busstop_hex_df,
             by = c("DESTIN_GRID" = "grid_id")) %>%
  rename(des_bs = ORIGIN_PT_CODE)

toporigin_bs <- data.frame(unique(toporigin_des$origin_bs)) %>% 
  rename(ORIGIN_PT_CODE = 1)

topdes_bs <- data.frame(unique(toporigin_des$des_bs))%>% 
  rename(ORIGIN_PT_CODE = 1)

topori_bsname <- left_join(toporigin_bs, valid_busstop)
topdes_bsname <- left_join(topdes_bs, valid_busstop)

ori <- toString(topori_bsname$LOC_DESC)
des <- toString(topdes_bsname$LOC_DESC)



mostpop <- data.frame((ori), (des)) %>%
  rename("Most popular origin bustops" = 1,
         "Most popular destination bustops" = 2)


datatable(mostpop)
```

We can see the bus stops that are found within the most popular origin-destination grid pair. Authorities can utilise the above information to plan for more buses to cater to those routes so as to ease traffic if needed.
:::

# Distance Matrix

Here we will begin calculating the distance between each hexagon. Normally people are less likely to take trips if the distance is longer.

We will create a hex data containing the grid id and the polygon information. Essentially this has the same information as the valid_hex variable that we have created earlier. But here we will add an additional step to filter out any duplicates.

```{r}
hex <- left_join( honeycomb_grid_sf,busstop_hex)%>%
  group_by(grid_id)%>%
  drop_na()%>%
  select(1)
hex<-unique(hex)

```

Next we will convert it to a *Spatial Polygonal Dataframe* object using the *as()* function.

```{r}
#| code-fold: false
hex_sp <- as(hex, "Spatial")
hex_sp
```

Next we will calculate the distance between the grids. The *spDists()* function will use the center of each hexagon for calculating the distance between grids. This way we are able to standardise the reference point of each grid to the other grids.

```{r}
#| code-fold: false
DIST <- spDists(hex_sp, 
                longlat = FALSE)
head(DIST, n=c(10, 10))
```

We will then rename the column and row names with their respective grid ids.

```{r}
#| code-fold: false
sz_names <- hex$grid_id

colnames(DIST) <- paste0(sz_names)
rownames(DIST) <- paste0(sz_names)
```

We can then pivot the dataframe to form a table for easier referencing.

```{r}
distPair <- melt(DIST) %>%
  rename(DIST = value)
head(distPair, 10)

```

Here we will substitute any distance of 0 with 50m as we will log our subsequent distance data, this will avoid log(0) errors.

```{R}
distPair %>%
  filter(DIST > 0) %>%
  summary()
distPair$DIST <- ifelse(distPair$DIST == 0,
                        50, distPair$DIST)
distPair %>%
  summary()
```

We can then rename the column names.

```{R}
distPair <- distPair %>%
  rename(orig = Var1,
         dest = Var2)
```

We will reassign the variable name for subsequent processing. We do this so as to avoid manipulating data that we do not want to modify. 

```{R}
flow_data <- OD_weekdayPM
```

Here we will join the distance between grids and their respective origin-destination grid pairs.

```{R}
flow_data1 <- flow_data %>%
  left_join (distPair,
             by = c("ORIGIN_GRID" = "orig",
                    "DESTIN_GRID" = "dest"))
```

## Plotting

```{R}
ggplot(data = flow_data1,
       aes(x = log(DIST),
           y = log(TRIPS))) +
  geom_point() +
  geom_smooth(method = lm)

```

We see that the graph shows an inverse relationship between trips and distance. This is expected as the further a place is, the less likely people would want to travel.

## Attractive and propulsive factor processing

Here we will prepare the data into attractive and propulsive sections. To differentiate that we will use _A for each variable and _P for variables used for attractive and propulsive respectively.

```{R}
#| code-fold: false
hex_A <- hex
hex_P <- hex
```

::: panel-tabset
## **Attractive**

Here we will assemble the attractive factors, which are number of estimated HDB households, retail stores MRT exits in each hexagon.

We will also be using the number of dwelling units as an estimation for the number of households staying in that HDB.

```{r}
#| code-fold: false
dwell_sf <- hdb_sf %>%
  select(13) 
datatable(dwell_sf)
```

Next we will join the number of households that belong to a grid with their respective grids.

```{R}
hex_hdb<- st_intersection(hex_A, dwell_sf)%>%
  group_by(grid_id) %>%
  summarise(total_dwelling_units = sum(total_dwelling_units))%>%
  rename(DWELL_COUNT = total_dwelling_units) %>%
  st_drop_geometry()

hex_A <- left_join(hex_A, hex_hdb) %>% 
  mutate(DWELL_COUNT = ifelse(is.na(DWELL_COUNT), 0, DWELL_COUNT))
```

Other information that we will include in our attractive factors include the number of residential estates and MRT exits.

```{r}
hex_A$retail_COUNT <- lengths(st_intersects(hex_A, retail_sf))  
hex_A$stn_exit_COUNT <- lengths(st_intersects(hex_A, stn_exit_sf))
```


::: panel-tabset
Here we will see if the top 100 most popular bus routes will intersect with the our attractive factors. Firstly we will obtain our top 100 most popular bus routes.

```{R}
top100 <- head(arrange(OD_weekdayPM, desc(TRIPS)), n = 100)
```

### HDB and trips made

We can look at the number of trips made to grids that contains HDBs. To do that we can first create grids with that contains HDBs. We will then check if the destination grid of our top 100 most popular trips have commonality with our grids that contains schools.

```{r}
hdb_grid <- hex_A%>%
  filter(DWELL_COUNT >0) %>%
  select(1,3) %>%
  rename(DESTIN_GRID = grid_id)%>%
  st_drop_geometry()

hdb_des <- left_join(top100, hdb_grid) %>%
  drop_na()

```

```{r}
tmap_options(check.and.fix = TRUE)  #polygon may not close, so need to auto close
tm_shape(mpsz)+  
  tm_polygons()+
  tm_shape(hex_A)+  
  
  tm_fill("DWELL_COUNT", 
          style = "pretty",
          palette = "Blues",
          title = "number of households") +
  tm_polygons()+
  
  
  tm_shape(hdb_sf)+  #biz layer
  tm_dots(alpha = 0.05)+
  
hdb_des %>%  

tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0,1,3,5,10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Common destination grid of top 100 trips and HDBs(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)



```

We see a clusters of residential areas across Singapore. We can also see that most of the popular bus routes contains HDBs as their destination, this may mean that residential areas serves as a point of attraction since people are going back home in the evening.

### Retail count and trips made

```{r}
retail_grid <- hex_A %>%
  filter(retail_COUNT >0)%>%
  select(1,4) %>%
  rename(DESTIN_GRID = grid_id)%>%
  st_drop_geometry()

retail_des <- left_join(top100, retail_grid) %>%
  drop_na()
summary(retail_des$retail_COUNT)


```

```{r}
tmap_options(check.and.fix = TRUE)  #polygon may not close, so need to auto close
tm_shape(mpsz)+  #sg boundary
  tm_polygons()+
  tm_shape(hex_A)+  #sg boundary
  # tm_polygons()+  #sg boundary
  
  tm_fill("retail_COUNT", 
          style = "pretty", 
          palette = "Reds",
          title = "number of retail outlets") +
  tm_polygons()+
  
  
  tm_shape(retail_sf)+
  tm_dots(alpha = 0.5)+
  
retail_des %>%  

tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0,1,3,5,10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Common destination grid of top 100 trips and retails(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)




```

From this data we can see that a retail outlets are generally quite widespread but can also be seen to cluster at the heartlands. Perhaps due to the widespread locations of retail outlets, we can see high commonality between the popular bus routes and retail locations.

### MRT exits and trips made

```{r}

mrt_grid <- hex_A %>%
  filter(stn_exit_COUNT >0)%>%
  select(1,5) %>%
  rename(DESTIN_GRID = grid_id)%>%
  st_drop_geometry()

mrt_des <- left_join(top100, mrt_grid) %>%
  drop_na()


```

```{r}

tmap_options(check.and.fix = TRUE)  
tm_shape(mpsz)+  #sg boundary
  tm_polygons()+
  tm_shape(hex_A)+ 
  
  tm_fill("stn_exit_COUNT", 
          style = "pretty",
          palette = "Greens",
          title = "number of MRT exits") +
  tm_polygons()+

    tm_shape(fin_sf)+  
  tm_dots(alpha = 0.1)+
  
mrt_des %>%  
 tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0,1,3,5,10),
           n = 6,
           alpha = 0.5)+
  tm_layout(main.title = "Common destination grid of top 100 trips and MRT exits(Weekday 5pm to 9pm)",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)



```

We can once again see that the heartlands contains the most number of MRT exits, this is likely from the number of larger train stations which services multiple lines, hence requiring more exits to prevent bottle-necking during rush hour. We see some commonality between train stations exits and our popular bus stop destinations.
:::
## **Propulsive**

We will match all of the number of schools, businesses and financial services to their respective grids.

```{R}
hex_P$SCHOOL_COUNT <- lengths(st_intersects(hex_P, schools_sf))
hex_P$BIZ_COUNT <- lengths(st_intersects(hex_P, biz_sf))
hex_P$FIN_COUNT <- lengths(st_intersects(hex_P, fin_sf))


```

```{r}
tmap_options(check.and.fix = TRUE)  
tm_shape(mpsz)+  #sg boundary
  tm_polygons()+
  tm_shape(hex_P)+ 
  tm_polygons()+
  

    tm_shape(schools_sf)+  
  tm_dots(col = "blue", alpha = 0.5)+
  tm_shape(biz_sf)+  
  tm_dots(col = "red", alpha = 0.3)+
  tm_shape(fin_sf)+  
  tm_dots(col = "green", alpha = 0.1)+
  

  tm_layout(main.title = "Schools, businesses and mrt exits distribution",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            
            frame = TRUE) +
  
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)+

  
  tm_layout(legend.position = c("left", "bottom"),
            legend.bg.color = "white",
            legend.frame = TRUE,
            legend.text.size = 0.7)
  




```
To avoid over concentrating this page with maps, we will combine the distribution of schools, businesses and financial services together. We can see the distribution of schools, businesses and financial services dotted in blue, red and green respectively. We can see concentration of businesses in the industrialised zones and businesses in the heartland regions.

:::

# Model calibration

In this section we will prepare our data for subsequent model calibration.

Since the hex variable still contains geometry information, which will prevent us from joining the various school and business information with ridership using **left_join**, we will drop it.

```{r}
hex_A <- st_drop_geometry(hex_A)
hex_P <- st_drop_geometry(hex_P)

```

We can then join the school and business count information to the ridership using left_join. For attractive factors, we will join it by destination grid since the amentities are located in the destination grid. For propulsive factors, we will use origin grid since they are factors that will push us from the origin.

```{r}
flow_A<- flow_data1 %>%
  left_join (hex_A,
             by = c("DESTIN_GRID" = "grid_id"))

flow_P<- flow_data1 %>%
  left_join (hex_P,
             by = c("ORIGIN_GRID" = "grid_id"))
```

Since we will log the data for modelling, we will have to convert any 0 or NA value data amongst the different factors into a positive number that is less than 1. We will use 0.99.

```{r}


flow_A$DWELL_COUNT <- ifelse(
  is.na(flow_A$DWELL_COUNT) | flow_A$DWELL_COUNT == 0,
  0.99, flow_A$DWELL_COUNT)

flow_A$stn_exit_COUNT <- ifelse(
  is.na(flow_A$stn_exit_COUNT) | flow_A$stn_exit_COUNT == 0,
  0.99, flow_A$stn_exit_COUNT)

flow_A$retail_COUNT <- ifelse(
  is.na(flow_A$retail_COUNT) | flow_A$retail_COUNT == 0,
  0.99, flow_A$retail_COUNT)


flow_P$BIZ_COUNT <- ifelse(
  is.na(flow_P$BIZ_COUNT) | flow_P$BIZ_COUNT == 0,
  0.99, flow_P$BIZ_COUNT)

flow_P$SCHOOL_COUNT <- ifelse(
  is.na(flow_P$SCHOOL_COUNT) | flow_P$SCHOOL_COUNT == 0,
  0.99, flow_P$SCHOOL_COUNT)

flow_P$FIN_COUNT <- ifelse(
  is.na(flow_P$FIN_COUNT) | flow_P$FIN_COUNT == 0,
  0.99, flow_P$FIN_COUNT)

```

We will also have to convert the grid names of the origin and destination into characters. We have been using numerals for naming them which would be recognised as integers by R.

```{r}
flow_A$ORIGIN_GRID <- as.character(flow_A$ORIGIN_GRID)
flow_A$DESTIN_GRID <- as.character(flow_A$DESTIN_GRID)

flow_P$ORIGIN_GRID <- as.character(flow_P$ORIGIN_GRID)
flow_P$DESTIN_GRID <- as.character(flow_P$DESTIN_GRID)
```

```{r}
flow_A$FlowNoIntra <- ifelse(
  flow_A$ORIGIN_GRID == flow_A$DESTIN_GRID, 0, flow_A$TRIPS)
flow_A$offset <- ifelse(
  flow_A$ORIGIN_GRID == flow_A$DESTIN_GRID, 0.000001, 1)

inter_zonal_flow_A <- flow_A %>%
  filter(FlowNoIntra > 0)




flow_P$FlowNoIntra <- ifelse(
  flow_P$ORIGIN_GRID == flow_P$DESTIN_GRID, 0, flow_P$TRIPS)
flow_P$offset <- ifelse(
  flow_P$ORIGIN_GRID == flow_P$DESTIN_GRID, 0.000001, 1)

inter_zonal_flow_P <- flow_P %>%
  filter(FlowNoIntra > 0)



```

To make sure we are able to capture all of our output, we will adjust the maximum print length into a large integer.

```{r}
options(max.print = 100000)

```

## Goodness-of-fit

Goodness-of-fit refers to how well sample data is able to fit the curve. To calculate that we will first have to create our own R-squared function.

```{r}
CalcRSquared <- function(observed, estimated){
  r<-cor(observed, estimated)
  R2<-r^2
  R2}

```

## Modelling calibration

Here we will begin our actual modelling for our different factors.

For attractive factors we will be using: - Origin constrained model - Unconstrained model - Doubly constrained model

For propulsive factors we will be using: - Destination constrained model - Unconstrained model - Doubly constrained model

### Attractive modelling

::: panel-tabset
#### Origin(Production) constrained model

```{R}


orcSIM_Poisson_A <- glm(formula = TRIPS ~
                        ORIGIN_GRID + 
                          #use attractiveness factors
                        log(DWELL_COUNT)+
                        log(retail_COUNT) +
                        # 
                        # log(stn_exit_COUNT)+
                        # log(HDB_COUNT)+
                        log(stn_exit_COUNT)+
                        
                        log(DIST) - 1,  #have - 1 to remove intersept, already constrain to origin
                      family = poisson(link = "log"),
                      data = inter_zonal_flow_A,
                      na.action = na.exclude)

summary(orcSIM_Poisson_A)

```

We can see that for all of the attractive attributes showing good attractive pull for transport there.

```{r}
CalcRSquared(orcSIM_Poisson_A$data$TRIPS, orcSIM_Poisson_A$fitted.values)

```

We have a R-square value of 0.432, which indicates that about 43% of variation can be explained by the model.

#### Unconstrained modelling

```{R}

uncSIM_A <- glm(formula = TRIPS ~ 
               
                        log(DWELL_COUNT)+
                        log(retail_COUNT) +
                        log(stn_exit_COUNT)+
                log(DIST),
              family = poisson(link = "log"),
              data = inter_zonal_flow_A,
              na.action = na.exclude)

uncSIM_A

```

```{r}
CalcRSquared(uncSIM_A$data$TRIPS, uncSIM_A$fitted.values)

```

We have a R-square value of 0.163, which indicates that only about 16% of variation can be explained by the unconstrained model.

#### Doubly Constrained modelling

```{r}

dbcSIM_Poisson_A <- glm(formula = TRIPS~
                        ORIGIN_GRID + 
                        DESTIN_GRID + 
                        log(DIST), 
                      family = poisson(link = "log"),
                      data = inter_zonal_flow_A,
                      na.action = na.exclude)
summary(dbcSIM_Poisson_A)

```

```{r}
CalcRSquared(dbcSIM_Poisson_A$data$TRIPS, dbcSIM_Poisson_A$fitted.values)

```

Here we have a R-square value of 0.586, which indicates that about 59% of variation can be explained by the doubly model, the best so far.
:::

### **Propulsive modelling**

::: panel-tabset
#### Destination constrained

```{R}

decSIM_P <- glm(formula = TRIPS ~ 
                DESTIN_GRID + 
                log(SCHOOL_COUNT) +
                log(BIZ_COUNT)+
                log(FIN_COUNT)+
                log(DIST),
              family = poisson(link = "log"),
              data = inter_zonal_flow_P,
              na.action = na.exclude)
summary(decSIM_P)



```

```{r}

CalcRSquared(decSIM_P$data$TRIPS, decSIM_P$fitted.values)

```

We have a R-square value of 0.356, which indicates that about 36% of variation can be explained by the destination constrained model.

#### Unconstrained modelling

```{R}
uncSIM_P <- glm(formula = TRIPS ~ 
                log(SCHOOL_COUNT) +
                log(BIZ_COUNT)+
                log(FIN_COUNT)+
                log(DIST),
              family = poisson(link = "log"),
              data = inter_zonal_flow_P,
              na.action = na.exclude)

uncSIM_P

```

```{r}
CalcRSquared(uncSIM_P$data$TRIPS, uncSIM_P$fitted.values)

```

We have a R-square value of only 0.174, which indicates that only about 17% of variation can be explained by the unconstrained model.

#### Doubly Constrained modelling

```{r}

dbcSIM_Poisson_P <- glm(formula = TRIPS~
                        ORIGIN_GRID + 
                        DESTIN_GRID + 
                        log(DIST), # no -1 cus no attractiveness btw origin and des
                      family = poisson(link = "log"),
                      data = inter_zonal_flow_P,
                      na.action = na.exclude)
summary(dbcSIM_Poisson_P)

```

```{r}
CalcRSquared(dbcSIM_Poisson_P$data$TRIPS, dbcSIM_Poisson_P$fitted.values)

```

This is the same model as the one performed for attractive factors. We have a R-square value of 0.586, which indicates that about 59% of variation can be explained by this model, similarly showing the best R-squared value for propulsive factors.
:::

# Model Comparison

To compare the suitability between different models, we can use the Root-mean-square deviation(RMSE). We can do that using **performance** package.

::: panel-tabset
## Attractive modelling comparison

We will first list all of the models used for attractive modelling.

```{r}

model_list_A <- list(unconstrained=uncSIM_A,
                   originConstrained=orcSIM_Poisson_A,
                   doublyConstrained=dbcSIM_Poisson_A)
```

We can then compare using the *compare_performance()* function of the **performance** package.

```{r}
compare_performance(model_list_A,
                    metrics = "RMSE")

```

From the different RMSE calculated above, we can see that the doubly constrained model has the lowest RMSE value of 1043.379, which represents the most accurate model out of the 3 tested models for attractive factors.

## Propulsive modelling

We will repeat the same steps for the different models for the propulsive models as well.

```{r}

model_list_P <- list(unconstrained=uncSIM_P,
                   destinationConstrained=decSIM_P,
                   doublyConstrained=dbcSIM_Poisson_P)
```

```{r}
compare_performance(model_list_P,
                    metrics = "RMSE")

```

Similarly, we see that the doubly constrained model shows the lowest RMSE value of 1043.379 which indicates that this modelling method has the best accuracy amongst the 3 tested models for propulsive factors.
:::

# Visualisation

Here we can visualise the modelling of the graphs between the observed and fitted values.

::: panel-tabset
## Attractive modelling

Firstly, we will obtain the fitted values from the different models followed by placing them back to the dataframe that contains the observed values.

::: panel-tabset
### Origin constrained

```{r}

df_A <- as.data.frame(orcSIM_Poisson_A$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow_A <- inter_zonal_flow_A %>%
  cbind(df_A)
```

### Unconstrained

```{r}
df_A <- as.data.frame(uncSIM_A$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow_A <- inter_zonal_flow_A %>%
  cbind(df_A)
```

### Doubly constrained

```{r}

df_A <- as.data.frame(dbcSIM_Poisson_A$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow_A <- inter_zonal_flow_A %>%
  cbind(df_A) 
```
:::

Here we will rename the columns for easier recognition.

```{r}
inter_zonal_flow_A <- inter_zonal_flow_A %>%
  rename(uncTRIPS = "uncSIM_A.fitted.values",
         orcTRIPS = "orcSIM_Poisson_A.fitted.values",
         dbcTRIPS = "dbcSIM_Poisson_A.fitted.values")

```

## Propulsive modelling

We will also do the same for the 3 propulsive models.

::: panel-tabset
### Destination constrained

```{r}

df_P <- as.data.frame(decSIM_P$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow_P <- inter_zonal_flow_P %>%
  cbind(df_P) 
```

### Unconstrained

```{r}
df_P <- as.data.frame(uncSIM_P$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow_P <- inter_zonal_flow_P %>%
  cbind(df_P)
```

### Doubly constrained

```{r}

df_P <- as.data.frame(dbcSIM_Poisson_P$fitted.values) %>%
  round(digits = 0)
inter_zonal_flow_P <- inter_zonal_flow_P %>%
  cbind(df_P) 
```
:::

```{r}
inter_zonal_flow_P <- inter_zonal_flow_P %>%
  rename(uncTRIPS = "uncSIM_P.fitted.values",
         decTRIPS = "decSIM_P.fitted.values",
         dbcTRIPS = "dbcSIM_Poisson_P.fitted.values")

```
:::

## Plotting

Finally we can then plot the 3 scatterplots using *geom_point()* from the **ggplot** package.

::: panel-tabset
### Attractive modelling

```{r}

unc_p_A <- ggplot(data = inter_zonal_flow_A,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

orc_p_A <- ggplot(data = inter_zonal_flow_A,
                aes(x = orcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)



dbc_p_A <- ggplot(data = inter_zonal_flow_A,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

ggarrange(unc_p_A, orc_p_A, dbc_p_A,
          ncol = 2,
          nrow = 2)
```
The plot shows the the observed(y-axis) and an estimated value(x-axis). Each observed and estimated pair trip value will form a dot on the graph. We can see that amongst the 3 plots, the plot from the doubly constrained model shows the best grouping, indicative of the accuracy from that model as seen previously. The unconstrained model shows the widest spread.

### Propulsive modelling

```{r}

unc_p_P <- ggplot(data = inter_zonal_flow_P,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)



dec_p_P <- ggplot(data = inter_zonal_flow_P,
                aes(x = decTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dbc_p_P <- ggplot(data = inter_zonal_flow_P,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

ggarrange(unc_p_P, dec_p_P, dbc_p_P,
          ncol = 2,
          nrow = 2)
```
Similarly for the propulsive factor modelling, the doubly constrained models show the tighests grouping and least difference between the observed and estimated trip values. Likewise the model with the widest spread is the unconstrained model.
:::
